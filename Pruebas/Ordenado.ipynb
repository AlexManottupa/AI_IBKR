{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LayerNormalization, MultiHeadAttention, Dropout, Dense, Reshape, Flatten\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from ib_insync import *\n",
    "import requests\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.startLoop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<IB connected to 127.0.0.1:7497 clientId=1>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ib = IB()\n",
    "ib.connect('127.0.0.1', 7497, clientId=1)  # Ajusta según tu configuración\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               open      high       low     close  volume  \\\n",
      "date                                                                        \n",
      "2024-10-09 16:15:00-05:00  1.307050  1.307260  1.306930  1.307150    -1.0   \n",
      "2024-10-09 16:20:00-05:00  1.307150  1.307185  1.307030  1.307030    -1.0   \n",
      "2024-10-09 16:25:00-05:00  1.307030  1.307480  1.307020  1.307215    -1.0   \n",
      "2024-10-09 16:30:00-05:00  1.307215  1.307215  1.307025  1.307080    -1.0   \n",
      "2024-10-09 16:35:00-05:00  1.307080  1.307100  1.306860  1.306895    -1.0   \n",
      "...                             ...       ...       ...       ...     ...   \n",
      "2024-11-19 16:45:00-05:00  1.268515  1.268520  1.268385  1.268390    -1.0   \n",
      "2024-11-19 16:50:00-05:00  1.268390  1.268405  1.268135  1.268230    -1.0   \n",
      "2024-11-19 16:55:00-05:00  1.268230  1.268280  1.268200  1.268235    -1.0   \n",
      "2024-11-19 17:15:00-05:00  1.268025  1.268130  1.268015  1.268050    -1.0   \n",
      "2024-11-19 17:20:00-05:00  1.268050  1.268050  1.268050  1.268050    -1.0   \n",
      "\n",
      "                           average  barCount  \n",
      "date                                          \n",
      "2024-10-09 16:15:00-05:00     -1.0        -1  \n",
      "2024-10-09 16:20:00-05:00     -1.0        -1  \n",
      "2024-10-09 16:25:00-05:00     -1.0        -1  \n",
      "2024-10-09 16:30:00-05:00     -1.0        -1  \n",
      "2024-10-09 16:35:00-05:00     -1.0        -1  \n",
      "...                            ...       ...  \n",
      "2024-11-19 16:45:00-05:00     -1.0        -1  \n",
      "2024-11-19 16:50:00-05:00     -1.0        -1  \n",
      "2024-11-19 16:55:00-05:00     -1.0        -1  \n",
      "2024-11-19 17:15:00-05:00     -1.0        -1  \n",
      "2024-11-19 17:20:00-05:00     -1.0        -1  \n",
      "\n",
      "[8267 rows x 7 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define el par de divisas\n",
    "symbol = Forex('GBPUSD')\n",
    "\n",
    "# Obtén los datos de los últimos 30 días en velas de 5 minutos\n",
    "end_date = datetime.datetime.now()\n",
    "start_date = end_date - datetime.timedelta(days=30)\n",
    "\n",
    "bars = ib.reqHistoricalData(\n",
    "    symbol,\n",
    "    endDateTime=end_date,\n",
    "    durationStr=\"30 D\",\n",
    "    barSizeSetting=\"5 mins\",\n",
    "    whatToShow=\"MIDPOINT\",\n",
    "    useRTH=True\n",
    ")\n",
    "\n",
    "# Convertir los datos a DataFrame\n",
    "df = pd.DataFrame(bars)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "df.set_index('date', inplace=True)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cálculo de indicadores técnicos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_technical_indicators(df):\n",
    "    df['SMA_10'] = df['close'].rolling(window=10).mean()\n",
    "    df['SMA_50'] = df['close'].rolling(window=50).mean()\n",
    "    df['RSI'] = 100 - (100 / (1 + df['close'].pct_change().add(1).rolling(window=14).mean()))\n",
    "    df['MACD'] = df['close'].ewm(span=12, adjust=False).mean() - df['close'].ewm(span=26, adjust=False).mean()\n",
    "    df['Volatility'] = df['close'].rolling(window=10).std()\n",
    "    return df\n",
    "\n",
    "df = calculate_technical_indicators(df)\n",
    "df.dropna(inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obtención de datos de noticias con News API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_API_KEY = 'b35c56d955ee45178c703f7f79c1dfca'\n",
    "\n",
    "def fetch_news(query, from_date, to_date):\n",
    "    url = f\"https://newsapi.org/v2/everything?q={query}&from={from_date}&to={to_date}&sortBy=popularity&apiKey={NEWS_API_KEY}\"\n",
    "    response = requests.get(url)\n",
    "    data = response.json()\n",
    "    return pd.DataFrame(data['articles'])\n",
    "\n",
    "news_df = fetch_news('GBP USD forex', start_date.strftime('%Y-%m-%d'), end_date.strftime('%Y-%m-%d'))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Procesa las noticias para extraer su polaridad:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vectorizar y transformar los títulos de las noticias\n",
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "tfidf = TfidfTransformer()\n",
    "\n",
    "# Ajustar la matriz TF-IDF\n",
    "news_tfidf = tfidf.fit_transform(vectorizer.fit_transform(news_df['title']))\n",
    "news_scores = news_tfidf.sum(axis=1).A.flatten()\n",
    "\n",
    "# Crear una columna 'news_sentiment' sincronizada con el índice de df\n",
    "# Asegúrate de unir las noticias con las fechas en df para que los índices coincidan\n",
    "news_df['date'] = pd.to_datetime(news_df['publishedAt']).dt.date  # Ajusta según el formato real de las fechas\n",
    "news_sentiment = pd.DataFrame({\n",
    "    'date': news_df['date'],\n",
    "    'sentiment': news_scores\n",
    "})\n",
    "\n",
    "# Promediar el sentimiento diario para unirlo con los datos históricos\n",
    "news_sentiment = news_sentiment.groupby('date').mean()\n",
    "\n",
    "# Convertir el índice de news_sentiment a pandas DateTimeIndex\n",
    "news_sentiment.index = pd.to_datetime(news_sentiment.index)\n",
    "\n",
    "# Asegurar que df tiene un índice de tipo DateTimeIndex para alinear\n",
    "df.index = pd.to_datetime(df.index)\n",
    "\n",
    "# Agregar el sentimiento promedio a df usando un join por fecha\n",
    "df['news_sentiment'] = df.index.map(lambda x: news_sentiment['sentiment'].get(x.date(), 0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "6. Simulación Monte Carlo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_simulation(data, num_simulations=10):\n",
    "    synthetic_data = []\n",
    "    for _ in range(num_simulations):\n",
    "        noise = np.random.normal(0, 0.02, size=len(data))\n",
    "        synthetic_data.append(data + noise)\n",
    "    return np.vstack(synthetic_data)\n",
    "\n",
    "synthetic_prices = monte_carlo_simulation(df['close'].values, num_simulations=10)\n",
    "synthetic_df = pd.DataFrame(synthetic_prices.T, columns=[f'sim_{i+1}' for i in range(10)], index=df.index)\n",
    "df = pd.concat([df, synthetic_df], axis=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Modelo Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define el modelo Transformer corregido\n",
    "def create_transformer(input_shape, output_steps):\n",
    "    inputs = Input(shape=input_shape)  # (sequence_length, features)\n",
    "    x = LayerNormalization(epsilon=1e-6)(inputs)\n",
    "    x = MultiHeadAttention(num_heads=4, key_dim=64)(x, x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = LayerNormalization(epsilon=1e-6)(x)\n",
    "    x = Flatten()(x)  # Aplana la salida\n",
    "    x = Dense(64, activation='relu')(x)\n",
    "    x = Dense(output_steps * 5, activation='linear')(x)  # Salida lineal para 24 pasos * 5 características\n",
    "    outputs = Reshape((output_steps, 5))(x)  # Redimensiona a (24, 5)\n",
    "    model = Model(inputs, outputs)\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "# Simulación de datos de ejemplo\n",
    "# df debería ser tu DataFrame original con datos financieros\n",
    "scaler = MinMaxScaler()\n",
    "scaled_data = scaler.fit_transform(df.values)\n",
    "\n",
    "# Crear secuencias de entrada (X) y salida (y)\n",
    "sequence_length = 60  # Últimas 60 velas como entrada\n",
    "output_steps = 24     # Predicción para 24 pasos (2 horas de datos de 5 minutos)\n",
    "X, y = [], []\n",
    "\n",
    "for i in range(sequence_length, len(scaled_data) - output_steps):\n",
    "    X.append(scaled_data[i - sequence_length:i])  # Ventana de entrada\n",
    "    y.append(scaled_data[i:i + output_steps, :5])  # Ventana de salida (5 características)\n",
    "\n",
    "X, y = np.array(X), np.array(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 14ms/step - loss: 0.2002 - mae: 0.3080 - val_loss: 0.0026 - val_mae: 0.0392\n",
      "Epoch 2/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0057 - mae: 0.0580 - val_loss: 0.0019 - val_mae: 0.0333\n",
      "Epoch 3/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0025 - mae: 0.0375 - val_loss: 0.0016 - val_mae: 0.0301\n",
      "Epoch 4/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0017 - mae: 0.0309 - val_loss: 0.0012 - val_mae: 0.0253\n",
      "Epoch 5/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0014 - mae: 0.0275 - val_loss: 0.0013 - val_mae: 0.0248\n",
      "Epoch 6/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 0.0012 - mae: 0.0259 - val_loss: 0.0015 - val_mae: 0.0252\n",
      "Epoch 7/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 0.0013 - mae: 0.0266 - val_loss: 0.0021 - val_mae: 0.0306\n",
      "Epoch 8/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 18ms/step - loss: 9.9471e-04 - mae: 0.0228 - val_loss: 0.0022 - val_mae: 0.0306\n",
      "Epoch 9/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 0.0010 - mae: 0.0232 - val_loss: 0.0016 - val_mae: 0.0264\n",
      "Epoch 10/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 8.2494e-04 - mae: 0.0207 - val_loss: 0.0021 - val_mae: 0.0320\n",
      "Epoch 11/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 9.1853e-04 - mae: 0.0218 - val_loss: 0.0018 - val_mae: 0.0281\n",
      "Epoch 12/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 8.7384e-04 - mae: 0.0212 - val_loss: 0.0014 - val_mae: 0.0243\n",
      "Epoch 13/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 7.9959e-04 - mae: 0.0203 - val_loss: 0.0017 - val_mae: 0.0278\n",
      "Epoch 14/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 8.1424e-04 - mae: 0.0206 - val_loss: 0.0014 - val_mae: 0.0239\n",
      "Epoch 15/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 7.7482e-04 - mae: 0.0200 - val_loss: 0.0015 - val_mae: 0.0259\n",
      "Epoch 16/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 17ms/step - loss: 7.6130e-04 - mae: 0.0197 - val_loss: 0.0013 - val_mae: 0.0231\n",
      "Epoch 17/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 7.4740e-04 - mae: 0.0195 - val_loss: 0.0025 - val_mae: 0.0352\n",
      "Epoch 18/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 16ms/step - loss: 7.3451e-04 - mae: 0.0194 - val_loss: 0.0014 - val_mae: 0.0250\n",
      "Epoch 19/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 15ms/step - loss: 6.6138e-04 - mae: 0.0183 - val_loss: 0.0023 - val_mae: 0.0340\n",
      "Epoch 20/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 15ms/step - loss: 6.2776e-04 - mae: 0.0178 - val_loss: 0.0025 - val_mae: 0.0362\n",
      "Epoch 21/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 6.8450e-04 - mae: 0.0187 - val_loss: 0.0015 - val_mae: 0.0266\n",
      "Epoch 22/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 5.9416e-04 - mae: 0.0172 - val_loss: 0.0030 - val_mae: 0.0404\n",
      "Epoch 23/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 6.0462e-04 - mae: 0.0175 - val_loss: 0.0036 - val_mae: 0.0451\n",
      "Epoch 24/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 13ms/step - loss: 5.5484e-04 - mae: 0.0167 - val_loss: 0.0025 - val_mae: 0.0366\n",
      "Epoch 25/25\n",
      "\u001b[1m228/228\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 14ms/step - loss: 5.5429e-04 - mae: 0.0168 - val_loss: 0.0022 - val_mae: 0.0330\n"
     ]
    }
   ],
   "source": [
    "# Crear y entrenar el modelo\n",
    "model = create_transformer(X.shape[1:], output_steps)\n",
    "\n",
    "history = model.fit(\n",
    "    X, y,\n",
    "    epochs=25,\n",
    "    batch_size=32,\n",
    "    validation_split=0.1\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   open high  low close news_sentiment                       date\n",
      "0   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:00:07.515273\n",
      "1   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:05:07.515273\n",
      "2   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:10:07.515273\n",
      "3   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:15:07.515273\n",
      "4   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:20:07.515273\n",
      "5   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:25:07.515273\n",
      "6   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:30:07.515273\n",
      "7   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:35:07.515273\n",
      "8   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:40:07.515273\n",
      "9   NaN  NaN  NaN   NaN            NaN 2024-11-19 18:45:07.515273\n",
      "10  NaN  NaN  NaN   NaN            NaN 2024-11-19 18:50:07.515273\n",
      "11  NaN  NaN  NaN   NaN            NaN 2024-11-19 18:55:07.515273\n",
      "12  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:00:07.515273\n",
      "13  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:05:07.515273\n",
      "14  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:10:07.515273\n",
      "15  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:15:07.515273\n",
      "16  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:20:07.515273\n",
      "17  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:25:07.515273\n",
      "18  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:30:07.515273\n",
      "19  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:35:07.515273\n",
      "20  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:40:07.515273\n",
      "21  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:45:07.515273\n",
      "22  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:50:07.515273\n",
      "23  NaN  NaN  NaN   NaN            NaN 2024-11-19 19:55:07.515273\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\alex_\\AppData\\Local\\Temp\\ipykernel_7816\\3367027681.py:7: FutureWarning: 'T' is deprecated and will be removed in a future version, please use 'min' instead.\n",
      "  predicted_dates = pd.date_range(start=end_date.replace(hour=18, minute=00), periods=24, freq='5T')\n"
     ]
    }
   ],
   "source": [
    "# Crear un DataFrame con las predicciones\n",
    "# Asegúrate de que las columnas coincidan con las 5 características (open, high, low, close, sentiment)\n",
    "result_df = pd.DataFrame( columns=['open', 'high', 'low', 'close', 'news_sentiment'])\n",
    "\n",
    "# Generar las fechas para las predicciones (24 predicciones cada 5 minutos)\n",
    "predicted_dates = pd.date_range(start=end_date.replace(hour=11, minute=0), periods=24, freq='5T')\n",
    "\n",
    "# Agregar las fechas al DataFrame\n",
    "result_df['date'] = predicted_dates\n",
    "\n",
    "# Mostrar las predicciones\n",
    "print(result_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "ib.disconnect()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
